{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- 20/12/2020 ------------\n",
    "## Editing Portfolio methods\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to create Profile of dataframe\n",
    "import pandas_profiling as pp\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from colorsStyles_displayOptions_Jup.ipynb\n"
     ]
    }
   ],
   "source": [
    "## Set Display options => import my file => 'colorsStyles_displayOptions.py'\n",
    "# To enable importing of jupyter notebook A into notebook B:\n",
    "import import_ipynb \n",
    "import colorsStyles_displayOptions_Jup as cstyle  # Set Display options\n",
    "cstyle.display_options_for_pandas()  \n",
    "\n",
    "# Import TypeConvertion\n",
    "import TypeConversions as tcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Visualize the plots in jupyter notebook:\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Advanced_EDA():\n",
    "    def __init__(self, df, df_name=None):\n",
    "        self.df = df\n",
    "\n",
    "        # self.df_name = df_name\n",
    "        self.portfolio = pd.DataFrame()\n",
    "        self.portfolio_rejected_variables = []\n",
    "        self.portfolio_subdf_duplicates = pd.DataFrame()\n",
    "        self.corr_features_to_drop = list()\n",
    "        # Create sub-folder named \"Portfolios\" in current directory Before running this code!\n",
    "        self.folder_path = r\".\\\\\"\n",
    "        return\n",
    "\n",
    "    # Dataframe Methods ------------------------------------\n",
    "    # Display Random Samples:\n",
    "    def Df_Get_Samples(self, samples_num, random=True):\n",
    "        if (random == True):\n",
    "            print (f\"Random {samples_num} samples:\")\n",
    "            display(self.df.sample(samples_num))\n",
    "        else:\n",
    "            print (f\"First {samples_num} samples:\")\n",
    "            display(self.df.head(samples_num))\n",
    "        return\n",
    "\n",
    "    def Df_get_basic_info(self):\n",
    "        print(f\"Dimensionality of the df: {self.df.shape}\")\n",
    "        print(f\"\\nColumns:{self.df.columns}\")\n",
    "        print(f\"\\nIndex name ': {self.df.index.name}'\")\n",
    "        print(f\"Index is Unique = {self.df.index.is_unique}\")\n",
    "        print(f\"Index type: {self.df.index.dtype}\")\n",
    "        if (self.df.index.is_unique == False): print(\"## If the Index is NOT Unique => check it's duplicates!! ##\")\n",
    "        self._Df_describtion()\n",
    "        return\n",
    "\n",
    "    def _Df_describtion(self):\n",
    "        print(\"\\nData describtion:\")\n",
    "        display(self.df.describe(include='all').T)  # Includes categorical variable, Excluding Nan values.\n",
    "        return\n",
    "\n",
    "    # Values Methods: (Non)Unique, Missing, Diversity ---------------------------------------\n",
    "    # 'None'           => for “missing” data of Pythons 'object' type (=\"empty value\")\n",
    "    # 'np.nan' [= NaN] => for “missing” Numerical data (=\"numerically invalid\")\n",
    "    def Val_Unique_per_column (self, without_None = True):\n",
    "        if (without_None):\n",
    "            print(\"\\nThe Number of Unique values ( Without 'None'):\")\n",
    "            display(self.df.nunique(dropna=True))\n",
    "\n",
    "            print(\"\\nThe Percentages(%) of all Unique values ( Without 'None') from total num of observations:\")\n",
    "            display(self.df.nunique(dropna=True) / self.df.shape[0] * 100)\n",
    "        else:\n",
    "            print(\"\\nThe Number of Unique values ( Including 'None'):\")\n",
    "            display( self.df.nunique(dropna=False))\n",
    "\n",
    "            print(\"\\nThe Percentages(%) of Unique values ( Including 'None') from total num of observations:\")\n",
    "            display(self.df.nunique(dropna=False) / self.df.shape[0] * 100 )\n",
    "        return\n",
    "\n",
    "    #   size()  - includes NaN (=> 'NaN'= values);------------------------------\n",
    "    #   count() - does not include NaN\n",
    "    def Val_Missing_with_percents_per_column (self):\n",
    "        print(\"CHECKING MISSING VALUES:\")\n",
    "        # Option 1 => is the complementary of Option 1:\n",
    "        # display(\"The Number of Non-null values (with duplicates) per feature: \\n\", self.df.count())\n",
    "        # Option 2 => is the complementary of Option 1:\n",
    "        print(\"The Number of NULL values per feature:\")\n",
    "        print(self.df.isnull().sum().sort_values())\n",
    "\n",
    "        print(\"\\nThe Percentages(%) of Non-null values (with duplicates!) from total num of observations (per feature):\")\n",
    "        print(self.df.count() / self.df.shape[0] * 100)\n",
    "\n",
    "        print(\"\\n ##!In the next step CHECK DUPLICATE observations (per feature) => \"\n",
    "              \"[ DUPLICATE observations = Total  observations - number of Unique values] ##\")\n",
    "        return\n",
    "\n",
    "    # Check Missing values and Diversity of values (=> for Target Variable: check if data is Imbalanced):\n",
    "    def Val_Missing_and_Diversity_for_specific_feature(self, col_name):\n",
    "        # Validation: Check if 'col_name' exists in current df:\n",
    "        if (col_name not in self.df.columns ):\n",
    "            print (f\"Column = {col_name} does not exist in current dataframe. Give a valid value.\")\n",
    "            return\n",
    "        \n",
    "        print(\"ANALYSIS OF VALUES IN '\", col_name, \"' COLUMN:\\n\")\n",
    "\n",
    "        df_without_None_col = self.df[~self.df[col_name].isna()]  # Sub-DataFrame Without 'NaN' (without missing values)\n",
    "        print(f\"The Total number of NON-None and Non-Unique in '{col_name}'column '{col_name}' = \", len(df_without_None_col))\n",
    "        print(f\"The Percent(%) of NON-None and Non-Unique values in '{col_name}' column'{col_name}' = \", \\\n",
    "              len(df_without_None_col) / len(self.df) *100 )\n",
    "\n",
    "        # Display diversity\n",
    "        self._Val_Diversity_for_list_of_wanted_Features(list_of_wanted_cols=[col_name])\n",
    "        # display(\"-\"*50)\n",
    "        \n",
    "        # If the values of 'col_name' are numeric, so build histogramm:\n",
    "        if ( np.issubdtype(self.df[col_name].dtype, np.number) == True):\n",
    "            # Number of bins:\n",
    "            bins_num = min(self.df[col_name].nunique(),100)            \n",
    "            self.df[col_name].hist(by=None, bins=bins_num )            \n",
    "        return\n",
    "\n",
    "        \n",
    "    # For each Feature => count it's different values:\n",
    "    # Usage: Check Diversity of values (=> for Target Variable: check if data is Imbalanced).\n",
    "    def _Val_Diversity_for_list_of_wanted_Features(self, list_of_wanted_cols):\n",
    "        # Check if 'list_of_wanted_cols' is a sub-list of df.columns:\n",
    "        if ( not set(list_of_wanted_cols).issubset(set(self.df.columns))):\n",
    "            print (\"Some/All columns in 'list_of_wanted_cols' does not exist in current dataframe. Check your input.\")\n",
    "            return\n",
    "        \n",
    "        for column in list_of_wanted_cols:\n",
    "            print(f\"\\nThe Diversity of values:\")\n",
    "            display(self.df[column].value_counts())  # count the number of rows for each feature\n",
    "            # Option 1 (the best) to calculate (%):\n",
    "            print(f\"\\nIn percents(%):\")\n",
    "            display(self.df[column].value_counts(normalize=True) * 100, \"(% type)\")\n",
    "            # Option 2 to calculate (%):\n",
    "            # display(f\"\\nIn percents(%): \\n  {self.df[column].value_counts() / self.df.shape[0] * 100} (% type)\")\n",
    "        return\n",
    "\n",
    "    # Very useful function!!!--------------------------------------------------------------------------------\n",
    "    # 1-Checks if values in wanted column are unique (including 'NaN')\n",
    "    #   It's important to check 'NaN' values, especially in situations when we have many empty values and some unique -\n",
    "    #    it will be mistake not to count NaN!\n",
    "    #\n",
    "    # 2-If values are Non-unique:\n",
    "    #      1) print sizes of groups (=\"group_size\") \n",
    "    #      2) print values for 3 biggest groups (by \"group_size\") => top to down\n",
    "    def Val_IfUnique_or_print_top_gropSizes(self, col_name, num_of_top_size_groups = 5):\n",
    "        # Validation: Check if 'col_name' exists in current df:\n",
    "        if (col_name not in self.df.columns ):\n",
    "            print (f\"Column = {col_name} does not exist in current dataframe. Give a valid value.\")\n",
    "            return\n",
    "        \n",
    "        # Counts how many value are in the column (including 'NaN' )\n",
    "        df_by_value_size = pd.DataFrame(self.df.groupby(col_name).size().rename(\"group_size\")).reset_index()\n",
    "        num_of_nan_in_col_name = self.df[self.df[col_name].isna()].shape[0]\n",
    "\n",
    "        if (len(df_by_value_size) == len(self.df)):\n",
    "            print(col_name, \" has UNIQUE values\")\n",
    "            return\n",
    "\n",
    "        elif ((len(df_by_value_size) + num_of_nan_in_col_name) == len(self.df)):\n",
    "            print(\"All existing values are Unique, BUT there is \", num_of_nan_in_col_name / self.df.shape[0] * 100,\n",
    "                  \"% of None values\")\n",
    "            return\n",
    "\n",
    "        print(col_name, \" has NON UNIQUE values\")\n",
    "\n",
    "        # For each value of \"group_size\" - count how many elements of the featcher are:\n",
    "        df_by_sizes_of_groups = (pd.DataFrame(df_by_value_size.groupby('group_size').count())  # index ='group_size'\n",
    "                                 .rename(columns={col_name: (\"num_of_diff_val_in_the_group\")})\n",
    "                                 .reset_index()\n",
    "                                 .sort_values(by='group_size', ascending=False)\n",
    "                                 )\n",
    "\n",
    "        # print all sizes of groups \n",
    "        print('\\nAll groups sizes:')\n",
    "        display(df_by_sizes_of_groups)\n",
    "\n",
    "        # size of the biggest group\n",
    "        print('\\nMax_group_size=')\n",
    "        display(df_by_sizes_of_groups['group_size'].max())  # <=> self.df.groupby(col_name).size().max()\n",
    "\n",
    "        # Set the number of Top values:\n",
    "        if (1 < len(df_by_sizes_of_groups) < num_of_top_size_groups):\n",
    "            num_of_top_size_groups = len(df_by_sizes_of_groups)  #- 1\n",
    "\n",
    "        # Display values for Top biggest groups (by \"group_size\") in top to down order\n",
    "        print(f'\\nTop {num_of_top_size_groups} Sizes of groups:')\n",
    "        df_top_sizes = df_by_sizes_of_groups.nlargest(n=num_of_top_size_groups, columns=\"group_size\")\n",
    "        display(df_top_sizes)\n",
    "\n",
    "        # list of Top biggest groups in top to down order:\n",
    "        print(\"Values of \", col_name, \" in each group of Top \", num_of_top_size_groups, \":\")\n",
    "\n",
    "        # run on reversed dataframe:\n",
    "        for i, row in df_by_sizes_of_groups.iterrows():\n",
    "            if (row['group_size'] in df_top_sizes['group_size'].tolist()):\n",
    "                display(df_by_value_size.groupby('group_size').get_group(row['group_size']))\n",
    "        return\n",
    "\n",
    "    # Column Types Methods ---------------------------------------------------------------------------\n",
    "    # More detailed info of columns: https://docs.scipy.org/doc/numpy/reference/arrays.scalars.html\n",
    "    def Col_Detailed_Dtypes(self, with_num_type_details = True):\n",
    "        print(\"\\nDETAILED COLUMNS TYPES:\\n\")\n",
    "        display(self.df.info())\n",
    "\n",
    "        # Numeric types (wih or without details):\n",
    "        print('-'*20, \"\\nDTYPE SUMMARY:\")\n",
    "        if (with_num_type_details):\n",
    "            self._Col_Numeric_details()\n",
    "        else:\n",
    "            # Numerical columns:\n",
    "            numeric_columns = list(self.df.select_dtypes(include=['number']).columns.values)\n",
    "            print(\"\\nNumeric_columns:\\n\", numeric_columns)\n",
    "\n",
    "        # Categorical columns:\n",
    "        category_columns = list(self.df.select_dtypes(include=['category']).columns.values)  # (exclude=[\"number\",\"bool_\",\"object_\"])\n",
    "        if (len(category_columns) >0): print(\"\\nCategory columns:\\n\", category_columns)\n",
    "\n",
    "        # Datetimes columns:\n",
    "        datetimes_columns = list(self.df.select_dtypes(include=['datetime']).columns.values)  # (exclude=[\"number\",\"bool_\",\"object_\"])\n",
    "        datetimes64_columns = list(self.df.select_dtypes(include=['datetime64']).columns.values)\n",
    "        if (len(datetimes_columns) >0): print(\"\\nDatetime columns:\\n\", datetimes_columns)\n",
    "        if (len(datetimes64_columns) >0): print(\"\\nDatetime64 columns:\\n\", datetimes64_columns)\n",
    "\n",
    "        # # Timedeltas columns:\n",
    "        timedelta_columns = list(self.df.select_dtypes(include=['timedelta']).columns.values)  # (exclude=[\"number\",\"bool_\",\"object_\"])\n",
    "        timedelta64_columns = list(self.df.select_dtypes(include=['timedelta64']).columns.values)\n",
    "        if (len(timedelta_columns) >0): print(\"\\nTimedelta columns:\\n\", timedelta_columns)\n",
    "        if (len(timedelta64_columns) >0): print(\"\\nTimedelta64 columns:\\n\", timedelta64_columns)\n",
    "\n",
    "        # Bool columns:\n",
    "        bool_columns = list(self.df.select_dtypes(include=['bool']).columns.values)  # (exclude=[\"number\",\"bool_\",\"object_\"])\n",
    "        if (len(bool_columns) >0): print(\"\\nBool columns:\\n\", bool_columns)\n",
    "\n",
    "        # Object columns:\n",
    "        obj_columns = list(self.df.select_dtypes(include=['object_']).columns.values)\n",
    "        if (len(obj_columns) >0): print(\"\\nSring columns:\\n\", obj_columns)\n",
    "        return  # numeric_columns, category_columns, obj_columns, datetimes_columns, bool_columns\n",
    "\n",
    "    def _Col_Numeric_details(self):\n",
    "        int64_columns = list(self.df[self.df.select_dtypes(include=[np.int64]).columns.values])\n",
    "        int_columns = list(self.df[self.df.select_dtypes(include=[np.int]).columns.values])\n",
    "#         int32_columns = list(self.df[self.df.select_dtypes(include=[np.int32]).columns.values]) # int32=int\n",
    "        float_columns = list(self.df[self.df.select_dtypes(include=[np.float]).columns.values])\n",
    "#         float64_columns = list(self.df[self.df.select_dtypes(include=[np.float64]).columns.values]) # float64=float\n",
    "        float32_columns = list(self.df[self.df.select_dtypes(include=[np.float32]).columns.values])\n",
    "\n",
    "        if (len(int64_columns) >0): print(\"\\nint64_columns:\\n\", int64_columns);\n",
    "#         if (len(float64_columns) >0): print(\"\\nfloat64_columns:\\n\", float64_columns)\n",
    "        if (len(float32_columns) >0): print(\"\\nfloat32_columns are:\\n\", float32_columns)\n",
    "        if (len(float_columns) >0): print(\"\\nfloat_columns:\\n\", float_columns)\n",
    "        if (len(int_columns) > 0): print(\"\\nint_columns:\\n\", int_columns);\n",
    "#         if (len(int32_columns) > 0): print(\"\\nint32_columns:\\n\", int32_columns);\n",
    "        return\n",
    "\n",
    "    # Sub-Dataframes Methods ----------------------------------------------------------------------------------\n",
    "    def get_SubDF_without_None_Val_for_specific_feature(self, col_name, returndf = False):\n",
    "        # Validation: Check if 'col_name' exists in current df:\n",
    "        if (col_name not in self.df.columns ):\n",
    "            print (f\"Column = {col_name} does not exist in current dataframe. Give a valid value.\")\n",
    "            return\n",
    "        \n",
    "        # print(\"-\" * 50)\n",
    "        # Create_new dataframe with Non-Empty 'location':\n",
    "        df_without_None_col = self.df[~self.df[col_name].isna()]\n",
    "        df_with_None_col = self.df[self.df[col_name].isna()]\n",
    "        print(f\"The Percent (%) of 'None' values in column '{col_name}' = \", len(df_with_None_col) / len(self.df)*100)\n",
    "\n",
    "        if returndf: return df_without_None_col, df_with_None_col\n",
    "        else:        return\n",
    "\n",
    "\n",
    "    # Display all duplicated rows with Specific Value in wanted column.\n",
    "    # If dropna=False => with None values. Othwerwise - without None\n",
    "    def get_SubDF_of_duplicates_for_specificVal_in_specificCol(self, col_name, checked_val, dropnav=False, \\\n",
    "                                                               returndf = False):\n",
    "        # Validation: Check if 'col_name' exists in current df:\n",
    "        if (col_name not in self.df.columns ):\n",
    "            print (f\"Column = {col_name} does not exist in current dataframe. Give a valid value.\")\n",
    "            return\n",
    "            \n",
    "        SubDF_with_specific_value = self.df[self.df[col_name] == checked_val]\n",
    "        \n",
    "        # Validation: Check if wanted value ('checked_val') exists in specified 'col_name':   \n",
    "        if (SubDF_with_specific_value.shape[0] == 0):\n",
    "            print( f\"Value '{checked_val}' does not exists in column '{col_name}'. Check another value.\")\n",
    "            return        \n",
    "        \n",
    "        print( f\"SUB-DF WITH VALUE ='{checked_val}' IN COLUMN '{col_name}':\\n\")\n",
    "        print(\"Sub-dataframes shape:\", SubDF_with_specific_value.shape)\n",
    "        display(\"Sub-dataframes describtion:\", SubDF_with_specific_value.describe().T)\n",
    "        display(\"Sample(5) of Sub-df:\")\n",
    "        display(SubDF_with_specific_value.sample(5))\n",
    "\n",
    "        if dropnav: display(\"Unique NON-None values in Sub-df:\", SubDF_with_specific_value.nunique(dropna=True))\n",
    "        else:       display(\"Unique values (with 'None') in Sub-df:\", SubDF_with_specific_value.nunique(dropna=False))\n",
    "\n",
    "        if returndf: return SubDF_with_specific_value\n",
    "        else: return\n",
    "        \n",
    "    \n",
    "    # NEW (12_09_2020) Correlation with visualisation ------------------------------------------------------\n",
    "    # Threshold=90 => for datasets with a small number of columns\n",
    "    # Threshold=80 => for datasets with a small number of columns\n",
    "    def Corr_with_plot_Find_and_remove (self, targetVal=None, corr_threshold = 0.90, remove_negative=False, \\\n",
    "                                               visualisation = True, return_subdf_without_corr_features = False):\n",
    "        print(\"Correlation with plot func\\n\" )\n",
    "        \n",
    "        # Drop 'targetVal' and check Correlation between features: \n",
    "        df_corr = pd.DataFrame()\n",
    "        if (targetVal is not None ) : \n",
    "            df_corr = self.df.drop(targetVal, axis=1).corr()  #Compute pairwise correlation of columns, excluding Nan values\n",
    "            display(df_corr.head(3))\n",
    "\n",
    "        if (visualisation == True):\n",
    "            self._Corr_visualisation(df_correl=df_corr, plot_title=\"Correlation plot for dataframe without targetVal\")\n",
    " \n",
    "        if (remove_negative == True):\n",
    "            df_corr = np.abs(df_corr)\n",
    "            # print(f\"1.2-corr_mat with absolute values: \\n{corr_mat.head(5)}\")\n",
    "\n",
    "        # Returns Lower triangle of an array => Diagonal (k=-1) and lower triangle are 'Nan'\n",
    "        lower_df = df_corr.where(np.tril(np.ones(df_corr.shape), k=-1).astype(np.bool))\n",
    "        #print(f\"lower_df: \\n{lower_df}\")\n",
    "\n",
    "        # Find features with correlation Greater than 'corr_threshold':\n",
    "        self.corr_features_to_drop = [column for column in lower_df.columns if any(lower_df[column] >= corr_threshold)]\n",
    "        print(f\"features_to_drop_list: \\n{self.corr_features_to_drop} \"\n",
    "              f\"\\nLenth of 'features_to_drop_list' = {len(self.corr_features_to_drop)} \")\n",
    "#               f\"\\nType of 'features_to_drop_list' = {type(self.corr_features_to_drop)}\")\n",
    "\n",
    "        # Drop correlative features(=columns) from the orig_df:\n",
    "        subdf_without_corr_features = self.df.drop(columns=self.corr_features_to_drop, axis=1, inplace=False, errors='raise')\n",
    "        \n",
    "        if (return_subdf_without_corr_features):  return subdf_without_corr_features\n",
    "        else: return\n",
    "        \n",
    "        \n",
    "    # The Best Visualisation that WORKS for both: Pycharm and NOTEBOOKs    \n",
    "    def _Corr_visualisation (self, df_correl, plot_title):            \n",
    "        # # 1- The Best Visualisation that WORKS for both: Pycharm and NOTEBOOKs:\n",
    "        fig, ax = plt.subplots(figsize=(30, 9))  # size of Graph's window\n",
    "        plt.subplots_adjust(top=0.9, left=0.1, right=1.2)  # ,  bottom=0.9, #right=0.05,  bottom=0.1,\n",
    "        corr = self.df.corr()\n",
    "        sns_plot = sns.heatmap(corr,\n",
    "                    mask=np.zeros_like(corr, dtype=np.bool),\n",
    "                    cmap=\"coolwarm\",  # 'RdBu_r' & 'BrBG' are other good diverging colormaps\n",
    "                    # square=True,\n",
    "                    ax=ax,\n",
    "                    annot=True,\n",
    "                    cbar=True,\n",
    "                    linewidths=.5)\n",
    "        plt.show()        \n",
    "        return\n",
    "    \n",
    "\n",
    "    # Porftolio Methods -------------------------- (.html)------------------------------------------\n",
    "    \"\"\"\n",
    "    ProfileReport was written by Simon Brugman (https://github.com/pandas-profiling/pandas-profiling).\n",
    "    It's very beautiful report for pandas dataframe and useful for analyzing the entire small/medium dataset properties, \n",
    "    and it can take some time to run it.\n",
    "    So when I need to analyze just specific features, I prefer to use other methods of my Advanced_EDA class.\n",
    "    \"\"\"\n",
    "   #  ProfileReport was written by Simon Brugman (https://github.com/pandas-profiling/pandas-profiling)\n",
    "    def Portf_Create(self, portf_name):\n",
    "        #  (This is a default configuration that disables expensive computations (such as correlations and dynamic binning))\n",
    "        self.profile = ProfileReport(self.df, title=\"Pandas Profiling Report\", \\\n",
    "                                     minimal=False,     #For LARGRE datasets (Option 1)\n",
    "                                     plot={'histogram': {'bins': 50}}, \\\n",
    "                                      html={'style': {'full_width': True}}, \\\n",
    "                                     sort=\"None\" )\n",
    "\n",
    "        # # For LARGRE datasets (Option 2): Generate report for 10000 data points\n",
    "        # self.profile = ProfileReport(data.sample(n = 10000),\n",
    "        #                 title=\"Titanic Data set\",\n",
    "        #                 html={'style': {'full_width': True}},\n",
    "        #                 sort=\"None\"\n",
    "\n",
    "        # 1-Save the portfolio in sub-folder named 'PORTFOLIOS' of the current folder:\n",
    "        file_path = self.folder_path + portf_name + '.html'\n",
    "        self.profile.to_file(output_file=file_path)\n",
    "        \n",
    "        # 2-Show in the window of the notebook (for Jupyter Notebook ONLY):\n",
    "        self.profile.to_widgets()\n",
    "        self.profile.to_notebook_iframe()\n",
    "        return\n",
    "\n",
    "  \n",
    "    #Get variables that are rejected for analysis (e.g. constant, mixed data types)\n",
    "    #Returns : a set of column names that are unsupported\n",
    "    def Portf_get_RegectedVariables(self, return_result = False ):\n",
    "        self.portfolio_rejected_variables = self.profile.get_rejected_variables()   \n",
    "        \n",
    "        print (\"Portfolio rejected variables:\")\n",
    "        display(self.portfolio_rejected_variables)\n",
    "        if (return_result): return self.portfolio_rejected_variables\n",
    "        else: return\n",
    "    \n",
    "    #Get duplicate rows and counts based on the configuration\n",
    "    #Returns: A DataFrame with the duplicate rows and their counts.\n",
    "    def Portf_get_Duplicates(self,return_result = False):\n",
    "        self.portfolio_subdf_duplicates = self.profile.get_duplicates()\n",
    "        \n",
    "        print (\"Portfolio duplicates (sub-df):\")\n",
    "        display( self.profile.get_duplicates())\n",
    "        if (return_result): return self.portfolio_subdf_duplicates\n",
    "        else: return\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **The report contains:** the type of columns, missing values, unique values, text analysis, and most frequent values.\n",
    "  \n",
    "* Currently the report recognizes the following types: Boolean, Numerical, Date, Categorical, URL,\n",
    "        Path, File and Image.\n",
    "  ** Future versions of pandas-profiling will have extended type support through 'visions'.\n",
    "    \n",
    "* **Quantile statistics:** minimum value, Q1, median, Q3, maximum, range, inter-quartile range.\n",
    "    \n",
    "* **Descriptive statistics:** mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation,\n",
    "                           kurtosis, skewness.\n",
    "    \n",
    "* **The Interaction section:** automatically generates interaction plots for every pair of variables.\n",
    "    \n",
    "* **Correlation Matrix (Heatmap) => FOR NUMERICAL VARIABLES ONLY!**     \n",
    "    VERY IMPORTANT! \n",
    "    Correlation Matrix (Heatmap) => FOR NUMERICAL VARIABLES ONLY!: a statistical technique \n",
    "        that can show whether and how strongly pairs of NUMERICAL VARIABLES are related.\n",
    "        It MUST MAKE SENSE!!!\n",
    "        \n",
    "    * The main result of a correlation is called the correlation coefficient (or “r”). \n",
    "    * It ranges from -1.0 to +1.0. \n",
    "    * The closer r is to +1 or -1, the more closely the two variables are related:\n",
    "         * If r is close to 0, it means there is no relationship between the variables. \n",
    "         * If r is positive, it means that as one variable gets larger the other gets larger. \n",
    "         * If r is negative it means that as one gets larger, the other gets smaller (an “inverse” correlation).\n",
    "    \n",
    "    If we have two CATEGORICAL VARIABLES that just coded as number => there is NO ANY SENSE,\n",
    "    because bigger number is not really bigger than previous, is's just a LABLE!\n",
    "    \n",
    "    * Pandas-profiling **auto-rejects the columns that are highly correlated** to previous columns => \n",
    "        => it's nice, but sometimes I would like to see it (=> feature request is opened)\n",
    "        \n",
    "    Dendrogram of missing values => a diagram that shows the hierarchical relationship between objects. \n",
    "        \n",
    "    For large datasets: \n",
    "        Option 1: Set 'minimal=True' OR\n",
    "        Option 2: Generate the profile report for a part of the data set => Set 'data.sample(n = 10000)'\n",
    "   \n",
    "    https://pandas-profiling.github.io/pandas-profiling/docs/master/profile_report.html\n",
    "    https://pandas-profiling.github.io/pandas-profiling/docs/master/index.html#getting-started\n",
    "    https://github.com/pandas-profiling/pandas-profiling/blob/master/src/pandas_profiling/config_default.yaml\n",
    "    https://towardsdatascience.com/accelerate-your-exploratory-data-analysis-with-pandas-profiling-4eca0cb770d1       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
